{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 266,
      "id": "be884c29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "be884c29",
        "outputId": "f3417f64-a944-47e8-cc38-49c5c34b3e9f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>.container { width:80% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "id": "USdrg7_mzoNV",
      "metadata": {
        "id": "USdrg7_mzoNV"
      },
      "outputs": [],
      "source": [
        "# !pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "id": "b2eea48b",
      "metadata": {
        "id": "b2eea48b"
      },
      "outputs": [],
      "source": [
        "# # In case of spellcheck\n",
        "# import spacy\n",
        "# import contextualSpellCheck #using Bert model\n",
        "# # The idea of using BERT was to use the context when correcting non-word error (NWE) .\n",
        "\n",
        "# nlp = spacy.load('en_core_web_sm')\n",
        "# contextualSpellCheck.add_to_pipe(nlp)\n",
        "# doc = nlp('Income was $9.4 milion compared to the prior year of $2.7 milion.')\n",
        "\n",
        "# print(doc._.performed_spellCheck) #Should be True\n",
        "# print(doc._.outcome_spellCheck) #Income was $9.4 million compared to the prior year of $2.7 million."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "id": "14ac3e86",
      "metadata": {
        "id": "14ac3e86"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import os\n",
        "import string\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as stopwords_spacy\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "# nltk.download('omw-1.4')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import contractions\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torch.utils.data\n",
        "import math\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "id": "fcd4fe29",
      "metadata": {
        "id": "fcd4fe29"
      },
      "outputs": [],
      "source": [
        "corpus_movie_conv = '/content/movie_conversations.txt'\n",
        "corpus_movie_lines = '/content/movie_lines.txt'\n",
        "max_len = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "id": "7irulk9Bup16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7irulk9Bup16",
        "outputId": "2f52a7a6-89f2-4fa9-c510-f7c2502396ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 272,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.path.isfile(corpus_movie_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "id": "ca72962b",
      "metadata": {
        "id": "ca72962b"
      },
      "outputs": [],
      "source": [
        "with open(corpus_movie_conv,\"r\", encoding='iso-8859-1') as f:\n",
        "  conv = f.readlines()\n",
        "with open(corpus_movie_lines,\"r\",encoding='iso-8859-1') as f:\n",
        "  lines=f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "id": "6dda6f1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dda6f1a",
        "outputId": "9e019b07-c459-4eee-ba25-dd56ed16c662"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\\n\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\\n\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\\n\"]"
            ]
          },
          "execution_count": 274,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conv[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "id": "83b4aa76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83b4aa76",
        "outputId": "ce94aa92-1f78-4bb5-f491-73d9b2629eec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n',\n",
              " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n',\n",
              " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n']"
            ]
          },
          "execution_count": 275,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "id": "164470f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "164470f7",
        "outputId": "5114613f-005f-4fbc-d06f-b9d6472aeaef"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'"
            ]
          },
          "execution_count": 276,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "id": "bff05914",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bff05914",
        "outputId": "b2f26a2e-0d4a-4bc2-f470-29e11b5e1383"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['L1045', 'u0', 'm0', 'BIANCA', 'They do not!\\n']"
            ]
          },
          "execution_count": 277,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines[0].split(\" +++$+++ \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "id": "3fb59b12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3fb59b12",
        "outputId": "8f1d4989-dacf-427c-f399-0f8536bf4280"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'They do not!\\n'"
            ]
          },
          "execution_count": 278,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines[0].split(\" +++$+++ \")[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "id": "834fcb75",
      "metadata": {
        "id": "834fcb75"
      },
      "outputs": [],
      "source": [
        "lines_dict={}\n",
        "for line in lines:\n",
        "    temp = line.split(\" +++$+++ \")\n",
        "    lines_dict[temp[0]]=temp[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "id": "654929e4",
      "metadata": {
        "id": "654929e4"
      },
      "outputs": [],
      "source": [
        "# # uncomment if you want to look at lines_dict\n",
        "# lines_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "id": "0faea819",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "0faea819",
        "outputId": "63698679-db50-43f2-929a-5717bf7b5f67"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\n'"
            ]
          },
          "execution_count": 281,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#we need to expand contractions\n",
        "lines_dict[\"L194\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "id": "07f65c0d",
      "metadata": {
        "id": "07f65c0d"
      },
      "outputs": [],
      "source": [
        "# import contractions\n",
        "# contractions.add('mychange', 'my change')\n",
        "contractions.add(\"'bout\",\"about\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "id": "6da59903",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6da59903",
        "outputId": "165bd872-14ee-4396-c728-b53e876d2c2e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'about'"
            ]
          },
          "execution_count": 283,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contractions.fix(\"'bout\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "id": "f192457f",
      "metadata": {
        "id": "f192457f"
      },
      "outputs": [],
      "source": [
        "# Define functions for preprocessing of text data\n",
        "def get_contraction(x):\n",
        "    return contractions.fix(x)\n",
        "def remove_punctuation(x):\n",
        "    return x.translate(str.maketrans('', '', string.punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "id": "3b5dbf87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b5dbf87",
        "outputId": "219548a6-c43f-4a93-dd31-a5881f500508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text:  Well, I thought we'd start with pronunciation, if that's okay with you.\n",
            "\n",
            "expanded text:  well, i thought we would start with pronunciation, if that is okay with you.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "print(\"Original text: \",lines_dict[\"L195\"])\n",
        "expand_sentence = get_contraction(lines_dict[\"L195\"]).lower()\n",
        "print(\"expanded text: \",expand_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "id": "24e2790d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24e2790d",
        "outputId": "7db964a2-86cd-4583-ea79-c6443c12028d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentence with punc:  well, i thought we would start with pronunciation, if that is okay with you.\n",
            "\n",
            "sentence without punc:  well i thought we would start with pronunciation if that is okay with you\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "print(\"sentence with punc: \",expand_sentence)\n",
        "print(\"sentence without punc: \",remove_punctuation(expand_sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "id": "a268a558",
      "metadata": {
        "id": "a268a558"
      },
      "outputs": [],
      "source": [
        "# expand contractions and remove the punctuations, also remove extra space in the sentence\n",
        "for key, value in lines_dict.items():\n",
        "    temp = remove_punctuation(get_contraction(value)).lower()\n",
        "    temp = temp.split()\n",
        "    temp = \" \".join(temp)\n",
        "    lines_dict[key]= temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "id": "a4fe7636",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "a4fe7636",
        "outputId": "e722fddd-153b-4139-fd03-e88fe5915c21"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'can we make this quick roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad again'"
            ]
          },
          "execution_count": 288,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# controlling the expanded contractions and punctuations\n",
        "lines_dict[\"L194\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "id": "b27d4a17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b27d4a17",
        "outputId": "4b23b19f-f12b-4c9f-e950-b786caf4916a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['L194', 'L195', 'L196', 'L197']"
            ]
          },
          "execution_count": 289,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We will eval the conversation texts\n",
        "eval(conv[0].split(\" +++$+++ \")[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "id": "a6441bca",
      "metadata": {
        "id": "a6441bca"
      },
      "outputs": [],
      "source": [
        "pairs = []\n",
        "for con in conv:\n",
        "    ids=eval(con.split(\" +++$+++ \")[-1])\n",
        "    for i in range(len(ids)):\n",
        "        question_and_pairs = []\n",
        "        if i == len(ids) - 1:\n",
        "            break\n",
        "        \n",
        "        first = lines_dict[ids[i]].strip()\n",
        "        second = lines_dict[ids[i+1]].strip()\n",
        "        question_and_pairs.append(first.split()[:max_len])\n",
        "        question_and_pairs.append(second.split()[:max_len])\n",
        "        \n",
        "        pairs.append(question_and_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "id": "588f5ba7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "588f5ba7",
        "outputId": "5fbcc0a5-4542-4610-ade9-47a21b06652f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have 221616 pairs, which we will train on\n"
          ]
        }
      ],
      "source": [
        "print(f\"We have {len(pairs)} pairs, which we will train on\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f91b2beb",
      "metadata": {
        "id": "f91b2beb"
      },
      "source": [
        "#### Now we create word to index dictionary\n",
        "- we will gather all the unique words in dataset and get counts of them.\n",
        "- we will remove words that occur less than five times, because we dont want to increase the number of vocabulary words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "id": "2afc0a04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2afc0a04",
        "outputId": "3b0ee7b0-7ff3-4350-b4d1-36d20c7cb85d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['can', 'we', 'make', 'this', 'quick', 'roxanne', 'korrine', 'and', 'andrew', 'barrett', 'are', 'having', 'an', 'incredibly', 'horrendous', 'public', 'break', 'up', 'on', 'the', 'quad', 'again'], ['well', 'i', 'thought', 'we', 'would', 'start', 'with', 'pronunciation', 'if', 'that', 'is', 'okay', 'with', 'you']]\n",
            "[['well', 'i', 'thought', 'we', 'would', 'start', 'with', 'pronunciation', 'if', 'that', 'is', 'okay', 'with', 'you'], ['not', 'the', 'hacking', 'and', 'gagging', 'and', 'spitting', 'part', 'please']]\n",
            "[['not', 'the', 'hacking', 'and', 'gagging', 'and', 'spitting', 'part', 'please'], ['okay', 'then', 'how', 'about', 'we', 'try', 'out', 'some', 'french', 'cuisine', 'saturday', 'night']]\n",
            "[['you', 'are', 'asking', 'me', 'out', 'that', 'is', 'so', 'cute', 'what', 'is', 'your', 'name', 'again'], ['forget', 'it']]\n"
          ]
        }
      ],
      "source": [
        "# look at question and answer conversation samples\n",
        "for v, pair in enumerate(pairs):\n",
        "    print(pair)\n",
        "    if v==3:\n",
        "        break   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "id": "e5e4f42a",
      "metadata": {
        "id": "e5e4f42a"
      },
      "outputs": [],
      "source": [
        "word_counts = Counter()\n",
        "for pair in pairs:\n",
        "#     we update word counts sequentially for each pair (conversation pair)\n",
        "    word_counts.update(pair[0]) #question\n",
        "    word_counts.update(pair[1]) #answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "id": "3146cfd7",
      "metadata": {
        "id": "3146cfd7"
      },
      "outputs": [],
      "source": [
        "# word_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "id": "c149e6cc",
      "metadata": {
        "id": "c149e6cc"
      },
      "outputs": [],
      "source": [
        "min_word_counts = 5\n",
        "words = [ word for word in word_counts.keys() if word_counts[word] > min_word_counts]\n",
        "words_map = { k:v+1 for v, k in enumerate(words)}# v=index, k=word , v+1 because start_index=1\n",
        "words_map[\"<unknown>\"]= len(words_map) +1 # unknown words is assigned to word which occurs less than five times\n",
        "words_map[\"<start>\"]= len(words_map) +1 \n",
        "words_map[\"<end>\"]= len(words_map) +1 \n",
        "words_map[\"<pad>\"]= 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "id": "f9a6b369",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9a6b369",
        "outputId": "22bb907a-e185-4e38-a8b9-9b2371ca8e62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total words are: 18039\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total words are: {len(words_map)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "id": "18c09c37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18c09c37",
        "outputId": "f3547232-3e53-4bd5-a64d-5f7be82f9661"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18038"
            ]
          },
          "execution_count": 297,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words_map[\"<end>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "id": "d716bfef",
      "metadata": {
        "id": "d716bfef"
      },
      "outputs": [],
      "source": [
        "with open(\"Wordmap_corpus.json\",\"w\") as f:\n",
        "    json.dump(words_map,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "id": "af708afd",
      "metadata": {
        "id": "af708afd"
      },
      "outputs": [],
      "source": [
        "def encode_question(words, words_map):\n",
        "    encoded_question = [words_map.get(word, words_map[\"unknown\"]) for word in words] + [words_map[\"<pad>\"]] * (max_len-len(words))\n",
        "    return encoded_question\n",
        "#     if word does not exits, then it will return words_map[\"unknown\"] value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "id": "a076defc",
      "metadata": {
        "id": "a076defc"
      },
      "outputs": [],
      "source": [
        "def encode_answer(words, words_map):\n",
        "    encoded_answer = [words_map[\"<start>\"]] + [words_map.get(word, words_map[\"unknown\"]) for word in words] + [words_map[\"<end>\"]] + [words_map[\"<pad>\"]] * (max_len-len(words))\n",
        "    return encoded_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "id": "20dd8695",
      "metadata": {
        "id": "20dd8695"
      },
      "outputs": [],
      "source": [
        "pairs_encoded = []\n",
        "for pair in pairs:\n",
        "    question = encode_question(pair[0], words_map)\n",
        "    answer = encode_answer(pair[1], words_map)\n",
        "    pairs_encoded.append([question, answer])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "id": "5539bf1e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5539bf1e",
        "outputId": "3381854d-0825-4eee-b693-8c1a52809437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1, 2, 3, 4, 5, 1624, 1624, 6, 7, 8, 9, 10, 11, 12, 1624, 13, 14, 15, 16, 17, 1624, 18, 0, 0, 0], [18037, 19, 20, 21, 2, 22, 23, 24, 1624, 25, 26, 27, 28, 24, 29, 18038, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ]
        }
      ],
      "source": [
        "# Visualize the first element of this matrix\n",
        "# <start> 18002\n",
        "# <end> 18003\n",
        "print(pairs_encoded[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "id": "005e50e5",
      "metadata": {
        "id": "005e50e5"
      },
      "outputs": [],
      "source": [
        "with open(\"pairs_encoded.json\",\"w\") as f:\n",
        "    json.dump(pairs_encoded, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "id": "847f69cc",
      "metadata": {
        "id": "847f69cc"
      },
      "outputs": [],
      "source": [
        "class Mydataset(Dataset):\n",
        "    def __init__(self):\n",
        "        path = \"/content/pairs_encoded.json\"\n",
        "        self.pairs = json.load(open(path))\n",
        "        self.dataset_size = len(self.pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "#         we need to convert this list containin numbers to a long tensor\n",
        "        question = torch.LongTensor(self.pairs[idx][0]) \n",
        "        answer = torch.LongTensor(self.pairs[idx][1]) # this will retrieve one element from your dataset\n",
        "        return question, answer\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.dataset_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "id": "babbb036",
      "metadata": {
        "id": "babbb036"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(Mydataset(),\n",
        "                                           batch_size = 100,\n",
        "                                          shuffle = True,\n",
        "                                          pin_memory = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "id": "ccf42d22",
      "metadata": {
        "id": "ccf42d22"
      },
      "outputs": [],
      "source": [
        "question, answer = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "id": "7ca703cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ca703cf",
        "outputId": "2f4df30a-2f34-4ec0-9ef2-711af07e44c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([100, 25]), torch.Size([100, 27]))"
            ]
          },
          "execution_count": 307,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question.shape, answer.shape\n",
        "# answer has length of 27, because it has <start> and<end> token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "id": "1b502fc9",
      "metadata": {
        "id": "1b502fc9"
      },
      "outputs": [],
      "source": [
        "# - sentence      : <start> I went to home last night <end>\n",
        "# - answer        : <start> I went to home last night , \"<start>\" is expected to predict \"I\", \"I\" is expected to predict \"went\", \"went\" is expected to predict \"to\"\n",
        "# - answer target :     I went to home last night <end>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "id": "d9f0ff82",
      "metadata": {
        "id": "d9f0ff82"
      },
      "outputs": [],
      "source": [
        "def create_masks(question, answer, answer_target):\n",
        "    \n",
        "    def subsequent_mask(size):\n",
        "        mask = torch.triu(torch.ones(size,size)).transpose(0,1).type(dtype=torch.uint8) \n",
        "        # since we are working with 4D tensor, we need to unsequeeze it\n",
        "        return mask.unsqueeze(0)\n",
        "    \n",
        "    question_mask = ((question !=0).int()).to(device)\n",
        "    question_mask = question_mask.unsqueeze(1).unsqueeze(1) # (batch_size, 1, 1, max_words)\n",
        "    \n",
        "    answer_mask = ((answer !=0).int()).to(device)\n",
        "    answer_mask = answer_mask.unsqueeze(1) # (batch_size, 1, max_words)\n",
        "    answer_mask = answer_mask & subsequent_mask(answer.size(-1)).type_as(answer_mask.data) # answer.size(-1) = max_words\n",
        "#     (batch_size, max_words, max_words)\n",
        "    answer_mask = answer_mask.unsqueeze(1)\n",
        "    answer_target_mask = (answer_target !=0).int()\n",
        "    \n",
        "    return question_mask, answer_mask, answer_target_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "id": "RxjrVHbk7u98",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxjrVHbk7u98",
        "outputId": "603ceaf0-2ded-4225-889f-9f913cd61af7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0], dtype=torch.int32)"
            ]
          },
          "execution_count": 310,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(question[0]!=0).int()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c0b869",
      "metadata": {
        "id": "b4c0b869"
      },
      "source": [
        "#### Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "id": "b2883558",
      "metadata": {
        "id": "b2883558"
      },
      "outputs": [],
      "source": [
        "# Create Embeddings\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, max_len=50):\n",
        "        super(Embeddings,self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_len = max_len\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.embed = nn.Embedding(self.vocab_size, self.embed_dim)\n",
        "        self.positional_encod = self.create_positional_encoding(self.max_len, self.embed_dim) # call function after creating\n",
        "        \n",
        "    def create_positional_encoding(self, max_len, embed_dim):\n",
        "        positional_encoding = torch.zeros(max_len, embed_dim).to(device)\n",
        "        for pos in range(max_len):\n",
        "            for i in range(0, embed_dim, 2):\n",
        "                positional_encoding[pos, i] = math.sin(pos / math.pow(10000,2*i/embed_dim) )\n",
        "                positional_encoding[pos, i+1] = math.cos(pos / math.pow(10000,2*i/embed_dim) )\n",
        "#                 we need to include batch size for this\n",
        "        positional_encoding = positional_encoding.unsqueeze(0) #results in shape (batch_size, max_len, embed_dim) --> (1, max_len, embed_dim),\n",
        "        return positional_encoding\n",
        "    \n",
        "    def forward(self, encoded_words):\n",
        "        embeddings = self.embed(encoded_words)*math.sqrt(self.embed_dim) # (batch_size, max_words, embed_dim)\n",
        "        # embeddings retrieve its embedding vector. These vectors will then be learnt as a parameters by the model,\n",
        "#         we wanna give more importance to the embeddings so we multiply is sqrt(self.embed_dim)\n",
        "#         max_words = embedding.size(1)\n",
        "        embeddings += self.positional_encod[:, :embeddings.size(1),] # pe will be automatically expanded to same batch_size as embeddings\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "id": "0e798d2e",
      "metadata": {
        "id": "0e798d2e"
      },
      "outputs": [],
      "source": [
        "class Multi_Head_Attention(nn.Module):\n",
        "    \n",
        "    def __init__(self, heads, model_dim): # model_dim or embed_dim\n",
        "        super(Multi_Head_Attention, self).__init__()\n",
        "        assert model_dim % heads == 0\n",
        "        self.heads = heads\n",
        "        self.model_dim = model_dim\n",
        "        self.d_k = self.model_dim // self.heads\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.query = nn.Linear(self.model_dim, self.model_dim)\n",
        "        self.key = nn.Linear(self.model_dim, self.model_dim)\n",
        "        self.value = nn.Linear(self.model_dim, self.model_dim)\n",
        "        self.concat = nn.Linear(self.model_dim, self.model_dim)\n",
        "    \n",
        "    def forward(self,query, key, value, mask):\n",
        "        query = self.query(query) # (batch_size, max_len, model_dim) ----> (batch_size, max_len, 512)\n",
        "        key = self.key(key) # (batch_size, max_len, model_dim)\n",
        "        value = self.value(value) # (batch_size, max_len, model_dim)\n",
        "        \n",
        "#         (batch_size, max_words, 512) ---> (batch_size, max_words, 8, 64) ---> (batch_size, 8, max_words, d_k)\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3) # result shape:(batch_size, 8, max_words, h, d_k) \n",
        "        key = key.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3) # result shape:(batch_size, 8, max_words, d_k)\n",
        "        value = value.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3) # result shape:(batch_size, 8, max_words, d_k)\n",
        "        \n",
        "#         (batch_size, 8, max_words, d_k) dot (batch_size, 8, d_k, max_words) == result shape:(batch_size, 8 , max_words, max_words)\n",
        "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(self.d_k)  # result shape:(batch_size, 8 , max_words, max_words)\n",
        "        scores = scores.masked_fill(mask ==0, -1e9)\n",
        "        weights = F.softmax(scores, dim = -1)\n",
        "        weights = self.dropout(weights) # result shape:(batch_size, 8, max_words, max_words)\n",
        "        \n",
        "#         (batch_size, 8, max_words, max_words) dot value (batch_size, 8, max_words, d_k) ------> result shape: (batch_size, 8, max_words, d_k)\n",
        "        context = torch.matmul(weights,value) # result shape: (batch_size, 8, max_words, d_k)\n",
        "#         transpose it from (batch_size, 8, max_words, d_k) to (batch_size, max_words, 8 , d_k) and concatenate them (batch_size, max_words, 8*d_k)\n",
        "        context = context.permute(0,2,1,3).contiguous() # result shape: (batch_size, max_words, 8 , d_k)\n",
        "        context = context.view(context.shape[0], -1, self.heads*self.d_k) # result shape: (batch_size, max_words or max_len, 512 or model_dim = 512)\n",
        "        context_interacted = self.concat(context)\n",
        "        \n",
        "        return context_interacted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "id": "1a32f352",
      "metadata": {
        "id": "1a32f352"
      },
      "outputs": [],
      "source": [
        "class Feed_Forward(nn.Module):\n",
        "    \n",
        "    def __init__(self,embed_dim, middle_dim = 2048):\n",
        "        super(Feed_Forward,self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.middle_dim = middle_dim\n",
        "        self.fc_linear1 = nn.Linear(self.embed_dim, self.middle_dim) #(512, 2048) --> result shape: 2048\n",
        "        self.fc_linear2 = nn.Linear(self.middle_dim, self.embed_dim) #(2048,512) --> result shape 512\n",
        "        self.dropout = nn.Dropout(0.1) # dropout rate used in the original paper\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.fc_linear1(x)\n",
        "        x = F.relu(x) # activation function used in the original paper\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc_linear2(x)\n",
        "        return x        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3c45e9f",
      "metadata": {
        "id": "f3c45e9f"
      },
      "source": [
        "#### d_model == embed_dim\n",
        "- In Encoder, we only have self_attention\n",
        "- In Decoder, we both have self_attention and source attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "id": "b1771025",
      "metadata": {
        "id": "b1771025"
      },
      "outputs": [],
      "source": [
        "class Encoder_layer(nn.Module):\n",
        "    \n",
        "    def __init__(self,embed_dim, heads):\n",
        "        super(Encoder_layer,self).__init__()\n",
        "        \n",
        "        self.self_multihead_attention = Multi_Head_Attention(heads, embed_dim)\n",
        "        self.feed_forward = Feed_Forward(embed_dim, middle_dim= 2048)\n",
        "        self.layernorm = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, embeddings, mask):\n",
        "        interacted_context = self.self_multihead_attention(embeddings, embeddings, embeddings, mask)\n",
        "#         in transformer, self_multihead_attention( query, key, value, mask) takes these values normally\n",
        "        interacted_dropout = self.dropout(interacted_context)\n",
        "        interacted_normalized = self.layernorm(interacted_dropout + embeddings)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted_normalized))\n",
        "        encoded_output = self.layernorm(feed_forward_out + interacted_normalized )\n",
        "        return encoded_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "id": "f12d8edd",
      "metadata": {
        "id": "f12d8edd"
      },
      "outputs": [],
      "source": [
        "class Decoder_layer(nn.Module):\n",
        "    \n",
        "    def __init__(self, embed_dim, heads):\n",
        "        super(Decoder_layer,self).__init__()\n",
        "        self.self_multihead_attention = Multi_Head_Attention(heads, embed_dim)\n",
        "        self.source_multihead_attention = Multi_Head_Attention(heads, embed_dim)\n",
        "        self.feed_forward = Feed_Forward(embed_dim, middle_dim= 2048)\n",
        "        self.layernorm = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "    \n",
        "    def forward(self, embeddings, encoded_output, source_mask, target_mask):\n",
        "        query = self.self_multihead_attention(embeddings, embeddings, embeddings, target_mask) # create query for source_attention\n",
        "        query = self.layernorm(query + embeddings) # outputs of the self attention plus the input, which is the embeddings\n",
        "#         keys and values as an input to source_attention comes from the encoded output\n",
        "        source_attention = self.source_multihead_attention(query, encoded_output, encoded_output, source_mask)\n",
        "        interacted_source_attention = self.dropout(source_attention)\n",
        "        interacted_source_attention = self.layernorm(interacted_source_attention + query)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted_source_attention))\n",
        "        decoded_output = self.layernorm(feed_forward_out + interacted_source_attention)\n",
        "        return decoded_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79926f0c",
      "metadata": {
        "id": "79926f0c"
      },
      "source": [
        "#### Build a full transformer model in py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 344,
      "id": "465f9106",
      "metadata": {
        "id": "465f9106"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    \n",
        "    def __init__(self, embed_dim, heads, num_layers, words_map):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.vocab_size = len(words_map)\n",
        "        self.embed  = Embeddings(self.vocab_size, self.embed_dim)\n",
        "#         N=num_layers / Encoder_layers are defined in original paper, N=6\n",
        "#         we have 6 Encoder_layers whose architerctures are the same but have different weights\n",
        "        self.encoder = nn.ModuleList([Encoder_layer(self.embed_dim, heads) for _ in range(num_layers)]) # assembles models in list\n",
        "        self.decoder = nn.ModuleList([Decoder_layer(self.embed_dim, heads) for _ in range(num_layers)])\n",
        "        self.logit = nn.Linear(self.embed_dim, self.vocab_size)\n",
        "        \n",
        "    def encode(self, source_words, source_mask): # source_words are questions\n",
        "        source_embeddings = self.embed(source_words)\n",
        "        for layer in self.encoder:\n",
        "            source_embeddings = layer(source_embeddings,source_mask)\n",
        "#             source_embeddings are fed into first encoder layer\n",
        "#             Here, there are 6 encoder layers, and output of the first encoder layer is fed into input of the second encoder layer and so it goes.\n",
        "        return source_embeddings\n",
        "\n",
        "    def decode(self, target_words, target_mask, source_embeddings, source_mask): # source_words are questions\n",
        "        target_embeddings = self.embed(target_words)\n",
        "        for layer in self.decoder:\n",
        "            target_embeddings = layer(target_embeddings, source_embeddings, source_mask, target_mask)\n",
        "#             source_embeddings are fed into first encoder layer\n",
        "#             Here, there are 6 encoder layers, and output of the first encoder layer is fed into input of the second encoder layer and so it goes.\n",
        "#             Here, both encoder and decoder share the same vocabulary and same words\n",
        "        return target_embeddings\n",
        "    \n",
        "    def forward(self, source_words, source_mask, target_words, target_mask):\n",
        "        encoded = self.encode(source_words, source_mask)\n",
        "        decoded = self.decode(target_words, target_mask, encoded, source_mask)\n",
        "#         why do we need log_softmax?\n",
        "#         we can ignore it if we train with crossentropy, we dont need to define log_softmax at all\n",
        "#         But here we use KL divergence loss, therefore we need to define softmax function\n",
        "        out = F.log_softmax(self.logit(decoded))\n",
        "#         -------------- additional information ---------------------\n",
        "#         pytorch automatically takes log and softmax of output logit if you use crossentropy\n",
        "#         out = self.logit(decoded)\n",
        "#         -----------------------------------------------------------\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "612d4a08",
      "metadata": {
        "id": "612d4a08"
      },
      "source": [
        "#### Now, we will define Adam optimization with warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 345,
      "id": "32dbcd4c",
      "metadata": {
        "id": "32dbcd4c"
      },
      "outputs": [],
      "source": [
        "class Adam_warmup:\n",
        "    \n",
        "    def __init__(self, model_size, warmup_steps, optimizer):\n",
        "        self.model_size = model_size\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.optimizer = optimizer\n",
        "        self.current_step = 0\n",
        "        self.lr = 0\n",
        "        \n",
        "    def get_lr(self):\n",
        "        return self.model_size**(-0.5)*min(self.current_step**(-0.5), self.current_step*self.warmup_steps**(-1.5))\n",
        "    \n",
        "    def step(self):\n",
        "        self.current_step += 1\n",
        "        lr = self.get_lr()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr   #update lr\n",
        "        self.lr = lr\n",
        "        self.optimizer.step() # update weights   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "712b83fd",
      "metadata": {
        "id": "712b83fd"
      },
      "source": [
        "- w < -- w - lr*gradient\n",
        "- optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 369,
      "id": "38261ae2",
      "metadata": {
        "id": "38261ae2"
      },
      "outputs": [],
      "source": [
        "class LosswithLS(nn.Module):\n",
        "    def __init__(self, size, smooth):\n",
        "        super(LosswithLS, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average= False, reduce= False) # The shape of matrix will be changed if reduce is True\n",
        "        self.smooth = smooth\n",
        "        self.confidence = 1 - self.smooth\n",
        "        self.size = size\n",
        "    \n",
        "    def forward(self, prediction, target, mask):\n",
        "        \"\"\"\n",
        "        prediction: (batch_size, max_words, vocab_size)\n",
        "        target and mask: (batch_size, max_words)\n",
        "        \"\"\"\n",
        "        prediction = prediction.view(-1,prediction.size(-1))\n",
        "        target = target.contiguous().view(-1)\n",
        "        mask = mask.float()\n",
        "        mask = mask.view(-1)\n",
        "        labels = prediction.data.clone()\n",
        "        labels.fill_(self.smooth/(self.size-1))\n",
        "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        loss = self.criterion(prediction, labels)  # shape (batch_size*max_words, vocab_size)\n",
        "        loss = (loss.sum(1)*mask).sum()/ mask.sum() # for example 1 0 1 1 0, ones are the words, 3/5\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e7baf4",
      "metadata": {
        "id": "31e7baf4"
      },
      "source": [
        "#### Visualize the operation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ba2fa5e",
      "metadata": {
        "id": "4ba2fa5e"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 370,
      "id": "6db06789",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6db06789",
        "outputId": "2eefa750-9237-4025-e183-24b7444ea79b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 3])"
            ]
          },
          "execution_count": 370,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size=2\n",
        "max_words=4\n",
        "vocab_size=3\n",
        "prediction = torch.randn(batch_size, max_words, vocab_size)\n",
        "prediction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 371,
      "id": "25b4407a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25b4407a",
        "outputId": "db565e81-cb87-4f1d-fce4-d595feaebb68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 3])"
            ]
          },
          "execution_count": 371,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction = prediction.view(-1, prediction.shape[-1])\n",
        "prediction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 372,
      "id": "c9844cfe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9844cfe",
        "outputId": "b2051cae-8cc8-43a9-9ebb-3f0e5921ea32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 0, 1, 0, 1, 2, 0, 1])"
            ]
          },
          "execution_count": 372,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target = torch.LongTensor(batch_size*max_words).random_(0,prediction.shape[-1])\n",
        "target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 373,
      "id": "5e1aa24b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e1aa24b",
        "outputId": "41825616-70ac-42d3-b4db-2719193178cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8])"
            ]
          },
          "execution_count": 373,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask= target!=0\n",
        "mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 374,
      "id": "09c2b798",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09c2b798",
        "outputId": "33c257c6-ae19-43cc-dfc4-29f7164b9d87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 3])"
            ]
          },
          "execution_count": 374,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = prediction.data.clone()\n",
        "labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "id": "cd3933f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd3933f5",
        "outputId": "bde32999-2244-4df7-c531-0cbaebf1f07f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.7088,  1.1347,  0.5537],\n",
              "        [-0.1594, -1.1809, -1.6327],\n",
              "        [ 0.0231, -0.7799,  0.8135],\n",
              "        [-0.4532, -0.5335,  0.6476],\n",
              "        [ 0.1810, -1.5008, -1.4220],\n",
              "        [ 0.5921, -1.8903, -1.5533],\n",
              "        [ 0.5950, -0.6280,  1.1034],\n",
              "        [-1.0888, -0.7710, -0.2193]])"
            ]
          },
          "execution_count": 375,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "id": "3eda081b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eda081b",
        "outputId": "5776b0e9-0591-472c-de8d-0682584eea37"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.9000, -0.9000, -0.9000],\n",
              "        [-0.9000, -0.9000, -0.9000],\n",
              "        [-0.9000, -0.9000, -0.9000],\n",
              "        [-0.9000, -0.9000, -0.9000],\n",
              "        [-0.9000, -0.9000, -0.9000],\n",
              "        [-0.9000, -0.9000, -0.9000],\n",
              "        [-0.9000, -0.9000, -0.9000],\n",
              "        [-0.9000, -0.9000, -0.9000]])"
            ]
          },
          "execution_count": 376,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels.fill_(0.3/3-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "id": "1aa7c062",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aa7c062",
        "outputId": "e310ecce-850b-46de-b91b-7d0749571c83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 0, 1, 0, 1, 2, 0, 1])"
            ]
          },
          "execution_count": 377,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "id": "977f4fb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "977f4fb4",
        "outputId": "88391b95-9b1d-41c7-a8ee-c9c146585ecd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.7000, -0.9000, -0.9000],\n",
              "        [ 0.7000, -0.9000, -0.9000],\n",
              "        [-0.9000,  0.7000, -0.9000],\n",
              "        [ 0.7000, -0.9000, -0.9000],\n",
              "        [-0.9000,  0.7000, -0.9000],\n",
              "        [-0.9000, -0.9000,  0.7000],\n",
              "        [ 0.7000, -0.9000, -0.9000],\n",
              "        [-0.9000,  0.7000, -0.9000]])"
            ]
          },
          "execution_count": 378,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels.scatter(1, target.data.unsqueeze(1), 1-0.3) # in order to use scatter, it has to be same dimension as target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "id": "101456f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "101456f9",
        "outputId": "cd61dd29-5340-4cf3-8188-252cedd323cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 3])"
            ]
          },
          "execution_count": 379,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = torch.randn(batch_size*max_words, vocab_size)\n",
        "loss.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "id": "3fff4418",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fff4418",
        "outputId": "900102d1-af37-4e6c-c1e6-36fe3e7b6a68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.1688,  0.7135, -0.7854],\n",
              "        [ 0.8839,  0.6079, -0.7647],\n",
              "        [ 0.7714, -0.2188,  1.7395],\n",
              "        [-0.6832,  1.0505, -0.0190],\n",
              "        [-0.3805, -0.7658, -1.0043],\n",
              "        [ 0.9719, -1.1071, -0.0734],\n",
              "        [ 1.9451, -0.0305, -0.1205],\n",
              "        [-0.8273,  0.5719, -0.3456]])"
            ]
          },
          "execution_count": 380,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "id": "9ada7231",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ada7231",
        "outputId": "0889802b-85e0-4f23-f065-bfb3587f203c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.2407,  0.7270,  2.2921,  0.3483, -2.1505, -0.2086,  1.7940, -0.6010])"
            ]
          },
          "execution_count": 381,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss.sum(1)  # we sum of the elements along the direction dim =1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "id": "4a2576f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a2576f9",
        "outputId": "4bc1871a-6340-4157-f0aa-f1d0e332bb0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([False, False,  True, False,  True,  True, False,  True])"
            ]
          },
          "execution_count": 382,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "id": "bd20f9e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd20f9e8",
        "outputId": "102d4ab8-2b74-4dae-97e2-91b52454febe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([8, 3]), torch.Size([8]))"
            ]
          },
          "execution_count": 383,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss.shape, mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "id": "e76aa081",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e76aa081",
        "outputId": "65c5ef19-9da1-4da7-e788-1280f215c237"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.0000,  0.0000,  2.2921,  0.0000, -2.1505, -0.2086,  0.0000, -0.6010])"
            ]
          },
          "execution_count": 384,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss.sum(1)*mask.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "id": "84a826d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84a826d8",
        "outputId": "d4aededa-51e8-427c-bc17-3812bfee88ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(-0.1670)"
            ]
          },
          "execution_count": 385,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(loss.sum(1)*mask.float()).sum() / mask.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7562ff1",
      "metadata": {
        "id": "b7562ff1"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "id": "d95751b1",
      "metadata": {
        "id": "d95751b1"
      },
      "outputs": [],
      "source": [
        "embed_dim = 512\n",
        "heads = 8\n",
        "num_layers = 6\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "epochs = 1\n",
        "\n",
        "with open(\"/content/Wordmap_corpus.json\",\"r\") as f:\n",
        "    words_map = json.load(f)\n",
        "    \n",
        "transformer = Transformer( embed_dim= embed_dim,heads= heads, num_layers= num_layers, words_map= words_map)\n",
        "transformer.to(device)\n",
        "\n",
        "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = Adam_warmup(embed_dim, warmup_steps=4000, optimizer=adam_optimizer)\n",
        "criterion = LosswithLS(size=len(words_map), smooth=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9902a932",
      "metadata": {
        "id": "9902a932"
      },
      "source": [
        "### Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "id": "2df6a493",
      "metadata": {
        "id": "2df6a493"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, transformer, criterion, epoch):\n",
        "    \n",
        "    transformer.train() # we need to set it training mode, because we are using dropout\n",
        "    # dropout acts differently in training and testing time.\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "    \n",
        "    for i, (question, answer) in enumerate(train_loader):\n",
        "        samples = question.shape[0] # retrieve the batch size\n",
        "        question = question.to(device)\n",
        "        answer = answer.to(device)\n",
        "        \n",
        "        # at a training time: for instance sentence: <start> A man is playing with a dog <end>  -----> Sentence\n",
        "        answer_input = answer[:,:-1] # last word is <end> token, we dont take it, example: <start> A man is playing with a dog -----> Input\n",
        "        answer_target = answer[:, 1:] # Beginning word is <start> token, we dont take it, example: A man is playing with a dog <end>  -----> Target\n",
        "        question_mask, answer_mask, answer_target_mask = create_masks(question, answer_input, answer_target)\n",
        "        \n",
        "        out = transformer(question, question_mask,answer_input, answer_mask) # now, we have our predictions\n",
        "        loss = criterion(out, answer_target, answer_target_mask)\n",
        "        #backpropagation\n",
        "        transformer_optimizer.optimizer.zero_grad() # we dont want to accumulate gradients\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "        sum_loss += loss.item() * samples # using item(), because we are getting value from tensor\n",
        "        count += samples # at the last batch for an epoch, the count = len(train_loader)*100\n",
        "        \n",
        "        if i % 100 ==0:\n",
        "            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss / count))\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eba0b1a",
      "metadata": {
        "id": "8eba0b1a"
      },
      "source": [
        "### Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 388,
      "id": "65a0b63c",
      "metadata": {
        "id": "65a0b63c"
      },
      "outputs": [],
      "source": [
        "def evaluate(transformer, question, question_mask, max_len, words_map):\n",
        "    \"\"\"\n",
        "    This function is gonna perform greedy search decoding with a batch size of 1\n",
        "    \"\"\"\n",
        "    rev_words_map = { index : word for word, index in words_map.items() }\n",
        "    transformer.eval()\n",
        "    start_token = words_map[\"<start>\"]\n",
        "#     def encode(self, source_words, source_mask):\n",
        "    encoded = transformer.encode(question, question_mask)\n",
        "    words = torch.LongTensor([[start_token]]).to(device) # shape: (1, 1)\n",
        "    \n",
        "    for step in range(max_len-1):\n",
        "        size = words.shape[1]\n",
        "        target_mask = torch.triu(torch.ones(size,size)).transpose(0,1).type(dtype=torch.uint8)\n",
        "        target_mask = target_mask.to(device).unsqueeze(0)\n",
        "#         def decode(self, target_words, target_mask, source_embeddings, source_mask):\n",
        "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
        "#         decoded shape : (batch_size, max_len, vocab_size) ---> (1, 1, vocab_size)\n",
        "        predictions = transformer.logit(decoded[:, -1]) # we only take the last element, outout of the work that you are about to generate\n",
        "#         predictions shape: (1, vocab_size)\n",
        "        _, next_word = torch.max(predictions, dim =1) # next_word shape:(1, 1)\n",
        "        next_word = next_word.item()\n",
        "        if next_word == words_map[\"<end>\"]:\n",
        "            break\n",
        "        words = torch.cat( [words, torch.LongTensor([[next_word]]).to(device)] , dim =1) #shape: (1, step+2)\n",
        "    \n",
        "    #words shape will be like (1, 5), so we will get rid of first dimension\n",
        "    words = words.squeeze(0) #resulting dimension is 1 dimensional tensor with 5 elements, each of which will be a predicted word.\n",
        "    words = words.tolist() #convert tensor to list\n",
        "    \n",
        "    # build sentence\n",
        "    sentence_idx = [w for w in words if w not in {words_map[\"<start>\"]}]\n",
        "    sentence = \" \".join([ rev_words_map[sentence_idx[i]] for i in range(len(sentence_idx)) ])\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 389,
      "id": "aqP9c6W65Zwp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqP9c6W65Zwp",
        "outputId": "d231f522-1ae9-4cb6-ddd4-d727f6fbf408"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 389,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86aa2673",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86aa2673",
        "outputId": "ed30073d-9a57-4bcd-d2de-9dffaac18161"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0][0/2217]\tLoss: 1.184\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "  train(train_loader, transformer, criterion, epoch)\n",
        "  state = {\"epoch\": epoch, \"transformer\" : transformer, \"transformer_optimizer\": transformer_optimizer}\n",
        "  torch.save(state, \"checpoint_\" + str(epoch) + \".tar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d76c6cc7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "d76c6cc7",
        "outputId": "de2015c9-2623-4cfc-eb1d-922b2c362b76"
      },
      "outputs": [],
      "source": [
        "# # after you train the transformer, you can choose checkpoint\n",
        "checkpoint = torch.load(\"checkpoint_1.tar\")\n",
        "transformer = checkpoint[\"transformer\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8385dd9",
      "metadata": {
        "id": "c8385dd9"
      },
      "outputs": [],
      "source": [
        "while(1):\n",
        "  question = input(\"Question: \")\n",
        "  if question == \"quit\":\n",
        "    break\n",
        "  \n",
        "  max_len = input(\"Enter max words to be generated: \")\n",
        "  encoded_qeustion = [ words_map.get(word, words_map[\"<unknown>\"]) for word in question.split()]\n",
        "  question = torch.LongTensor(encoded_qeustion).to(device).unsqueeze(0)\n",
        "  question = (question !=0).to(device).unsqueeze(1).unsqueeze(1)\n",
        "  sentence = evaluate(transformer, question, question_mask, int(max_len),words_map)\n",
        "  print(sentence)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 ('pythonProject')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "49a989f7cfe4b8da2a59a725bbf71baebbe1b42e64c745a96da729d2da270941"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
