{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "be884c29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "be884c29",
        "outputId": "41b60a10-9c51-48ee-ec7d-8eeac33e1f29"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>.container { width:80% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b2eea48b",
      "metadata": {
        "id": "b2eea48b"
      },
      "outputs": [],
      "source": [
        "# # In case of spellcheck\n",
        "# import spacy\n",
        "# import contextualSpellCheck #using Bert model\n",
        "# # The idea of using BERT was to use the context when correcting non-word error (NWE) .\n",
        "\n",
        "# nlp = spacy.load('en_core_web_sm')\n",
        "# contextualSpellCheck.add_to_pipe(nlp)\n",
        "# doc = nlp('Income was $9.4 milion compared to the prior year of $2.7 milion.')\n",
        "\n",
        "# print(doc._.performed_spellCheck) #Should be True\n",
        "# print(doc._.outcome_spellCheck) #Income was $9.4 million compared to the prior year of $2.7 million."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "X9VIoLxWCsSl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9VIoLxWCsSl",
        "outputId": "8c350108-c22e-4a79-d71c-d12692947a57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 13.5 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 66.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "14ac3e86",
      "metadata": {
        "id": "14ac3e86"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import os\n",
        "import string\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as stopwords_spacy\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "# nltk.download('omw-1.4')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import contractions\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torch.utils.data\n",
        "import math\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fcd4fe29",
      "metadata": {
        "id": "fcd4fe29"
      },
      "outputs": [],
      "source": [
        "corpus_movie_conv = '/content/movie_conversations.txt'\n",
        "corpus_movie_lines = '/content/movie_lines.txt'\n",
        "max_len = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ca72962b",
      "metadata": {
        "id": "ca72962b"
      },
      "outputs": [],
      "source": [
        "with open(corpus_movie_conv,\"r\", encoding='iso-8859-1') as f:\n",
        "  conv = f.readlines()\n",
        "with open(corpus_movie_lines,\"r\",encoding='iso-8859-1') as f:\n",
        "  lines=f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6dda6f1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dda6f1a",
        "outputId": "4e2d2069-2865-43a1-bc8a-02f0ae76cce3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\\n\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\\n\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\\n\"]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conv[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "83b4aa76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83b4aa76",
        "outputId": "f31f4cbd-d5c1-4342-f61e-f673914365f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n',\n",
              " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n',\n",
              " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "164470f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "164470f7",
        "outputId": "b27962cf-b23b-46b3-e983-f6244a063acc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bff05914",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bff05914",
        "outputId": "8f38600c-4f89-4eeb-95bd-c82d1ea04fc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['L1045', 'u0', 'm0', 'BIANCA', 'They do not!\\n']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines[0].split(\" +++$+++ \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3fb59b12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3fb59b12",
        "outputId": "36942d9e-709b-40de-f7b0-70423ebcb92e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'They do not!\\n'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines[0].split(\" +++$+++ \")[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "834fcb75",
      "metadata": {
        "id": "834fcb75"
      },
      "outputs": [],
      "source": [
        "lines_dict={}\n",
        "for line in lines:\n",
        "    temp = line.split(\" +++$+++ \")\n",
        "    lines_dict[temp[0]]=temp[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "654929e4",
      "metadata": {
        "id": "654929e4"
      },
      "outputs": [],
      "source": [
        "# # uncomment if you want to look at lines_dict\n",
        "# lines_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "0faea819",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0faea819",
        "outputId": "7ca103b4-3128-4c9c-8bc4-20cd092ba787"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\n'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#we need to expand contractions\n",
        "lines_dict[\"L194\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "07f65c0d",
      "metadata": {
        "id": "07f65c0d"
      },
      "outputs": [],
      "source": [
        "# import contractions\n",
        "# contractions.add('mychange', 'my change')\n",
        "contractions.add(\"'bout\",\"about\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "6da59903",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6da59903",
        "outputId": "90e965fb-b2ec-485c-cfc3-9a49c40af427"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'about'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contractions.fix(\"'bout\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f192457f",
      "metadata": {
        "id": "f192457f"
      },
      "outputs": [],
      "source": [
        "# Define functions for preprocessing of text data\n",
        "def get_contraction(x):\n",
        "    return contractions.fix(x)\n",
        "def remove_punctuation(x):\n",
        "    return x.translate(str.maketrans('', '', string.punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "3b5dbf87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b5dbf87",
        "outputId": "65acf05c-d0ee-47f1-d1c9-3e969e28cf17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text:  Well, I thought we'd start with pronunciation, if that's okay with you.\n",
            "\n",
            "expanded text:  well, i thought we would start with pronunciation, if that is okay with you.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "print(\"Original text: \",lines_dict[\"L195\"])\n",
        "expand_sentence = get_contraction(lines_dict[\"L195\"]).lower()\n",
        "print(\"expanded text: \",expand_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "24e2790d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24e2790d",
        "outputId": "1feea36b-0f81-436c-c1db-5e783cb11913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentence with punc:  well, i thought we would start with pronunciation, if that is okay with you.\n",
            "\n",
            "sentence without punc:  well i thought we would start with pronunciation if that is okay with you\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "print(\"sentence with punc: \",expand_sentence)\n",
        "print(\"sentence without punc: \",remove_punctuation(expand_sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a268a558",
      "metadata": {
        "id": "a268a558"
      },
      "outputs": [],
      "source": [
        "# expand contractions and remove the punctuations, also remove extra space in the sentence\n",
        "for key, value in lines_dict.items():\n",
        "    temp = remove_punctuation(get_contraction(value)).lower()\n",
        "    temp = temp.split()\n",
        "    temp = \" \".join(temp)\n",
        "    lines_dict[key]= temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a4fe7636",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a4fe7636",
        "outputId": "23ab11f6-d0ff-4a66-f8e4-192196362a48"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'can we make this quick roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad again'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# controlling the expanded contractions and punctuations\n",
        "lines_dict[\"L194\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "b27d4a17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b27d4a17",
        "outputId": "5acdb9ab-bccf-4ad6-e65d-8528f31440d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['L194', 'L195', 'L196', 'L197']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We will eval the conversation texts\n",
        "eval(conv[0].split(\" +++$+++ \")[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a6441bca",
      "metadata": {
        "id": "a6441bca"
      },
      "outputs": [],
      "source": [
        "pairs = []\n",
        "for con in conv:\n",
        "    ids=eval(con.split(\" +++$+++ \")[-1])\n",
        "    for i in range(len(ids)):\n",
        "        question_and_pairs = []\n",
        "        if i == len(ids) - 1:\n",
        "            break\n",
        "        \n",
        "        first = lines_dict[ids[i]].strip()\n",
        "        second = lines_dict[ids[i+1]].strip()\n",
        "        question_and_pairs.append(first.split()[:max_len])\n",
        "        question_and_pairs.append(second.split()[:max_len])\n",
        "        \n",
        "        pairs.append(question_and_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "588f5ba7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "588f5ba7",
        "outputId": "6b93f2f1-f78c-4e5a-dda6-41b839a17f87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have 221616 pairs, which we will train on\n"
          ]
        }
      ],
      "source": [
        "print(f\"We have {len(pairs)} pairs, which we will train on\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f91b2beb",
      "metadata": {
        "id": "f91b2beb"
      },
      "source": [
        "#### Now we create word to index dictionary\n",
        "- we will gather all the unique words in dataset and get counts of them.\n",
        "- we will remove words that occur less than five times, because we dont want to increase the number of vocabulary words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "2afc0a04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2afc0a04",
        "outputId": "cc47d219-46f5-415a-a6a3-95c1dc052d9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['can', 'we', 'make', 'this', 'quick', 'roxanne', 'korrine', 'and', 'andrew', 'barrett', 'are', 'having', 'an', 'incredibly', 'horrendous', 'public', 'break', 'up', 'on', 'the', 'quad', 'again'], ['well', 'i', 'thought', 'we', 'would', 'start', 'with', 'pronunciation', 'if', 'that', 'is', 'okay', 'with', 'you']]\n",
            "[['well', 'i', 'thought', 'we', 'would', 'start', 'with', 'pronunciation', 'if', 'that', 'is', 'okay', 'with', 'you'], ['not', 'the', 'hacking', 'and', 'gagging', 'and', 'spitting', 'part', 'please']]\n",
            "[['not', 'the', 'hacking', 'and', 'gagging', 'and', 'spitting', 'part', 'please'], ['okay', 'then', 'how', 'about', 'we', 'try', 'out', 'some', 'french', 'cuisine', 'saturday', 'night']]\n",
            "[['you', 'are', 'asking', 'me', 'out', 'that', 'is', 'so', 'cute', 'what', 'is', 'your', 'name', 'again'], ['forget', 'it']]\n"
          ]
        }
      ],
      "source": [
        "# look at question and answer conversation samples\n",
        "for v, pair in enumerate(pairs):\n",
        "    print(pair)\n",
        "    if v==3:\n",
        "        break   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "e5e4f42a",
      "metadata": {
        "id": "e5e4f42a"
      },
      "outputs": [],
      "source": [
        "word_counts = Counter()\n",
        "for pair in pairs:\n",
        "#     we update word counts sequentially for each pair (conversation pair)\n",
        "    word_counts.update(pair[0]) #question\n",
        "    word_counts.update(pair[1]) #answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "3146cfd7",
      "metadata": {
        "id": "3146cfd7"
      },
      "outputs": [],
      "source": [
        "# word_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "c149e6cc",
      "metadata": {
        "id": "c149e6cc"
      },
      "outputs": [],
      "source": [
        "min_word_counts = 5\n",
        "words = [ word for word in word_counts.keys() if word_counts[word] > min_word_counts]\n",
        "words_map = { k:v+1 for v, k in enumerate(words)}# v=index, k=word , v+1 because start_index=1\n",
        "words_map[\"<unknown>\"]= len(words_map) +1 # unknown words is assigned to word which occurs less than five times\n",
        "words_map[\"<start>\"]= len(words_map) +1 \n",
        "words_map[\"<end>\"]= len(words_map) +1 \n",
        "words_map[\"<pad>\"]= 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "f9a6b369",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9a6b369",
        "outputId": "0d910a55-3072-4427-e9ff-2027949b59ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total words are: 18039\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total words are: {len(words_map)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "18c09c37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18c09c37",
        "outputId": "5cf20bb5-7c9a-4238-90c0-474721313638"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18038"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words_map[\"<end>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "d716bfef",
      "metadata": {
        "id": "d716bfef"
      },
      "outputs": [],
      "source": [
        "with open(\"Wordmap_corpus.json\",\"w\") as f:\n",
        "    json.dump(words_map,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "af708afd",
      "metadata": {
        "id": "af708afd"
      },
      "outputs": [],
      "source": [
        "def encode_question(words, words_map):\n",
        "    encoded_question = [words_map.get(word, words_map[\"unknown\"]) for word in words] + [words_map[\"<pad>\"]] * (max_len-len(words))\n",
        "    return encoded_question\n",
        "#     if word does not exits, then it will return words_map[\"unknown\"] value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "a076defc",
      "metadata": {
        "id": "a076defc"
      },
      "outputs": [],
      "source": [
        "def encode_answer(words, words_map):\n",
        "    encoded_answer = [words_map[\"<start>\"]] + [words_map.get(word, words_map[\"unknown\"]) for word in words] + [words_map[\"<end>\"]] + [words_map[\"<pad>\"]] * (max_len-len(words))\n",
        "    return encoded_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "20dd8695",
      "metadata": {
        "id": "20dd8695"
      },
      "outputs": [],
      "source": [
        "pairs_encoded = []\n",
        "for pair in pairs:\n",
        "    question = encode_question(pair[0], words_map)\n",
        "    answer = encode_answer(pair[1], words_map)\n",
        "    pairs_encoded.append([question, answer])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "5539bf1e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5539bf1e",
        "outputId": "158ca2cb-dc19-440c-894d-0e51eddae4a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1, 2, 3, 4, 5, 1624, 1624, 6, 7, 8, 9, 10, 11, 12, 1624, 13, 14, 15, 16, 17, 1624, 18, 0, 0, 0], [18037, 19, 20, 21, 2, 22, 23, 24, 1624, 25, 26, 27, 28, 24, 29, 18038, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ]
        }
      ],
      "source": [
        "# Visualize the first element of this matrix\n",
        "# <start> 18002\n",
        "# <end> 18003\n",
        "print(pairs_encoded[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "005e50e5",
      "metadata": {
        "id": "005e50e5"
      },
      "outputs": [],
      "source": [
        "with open(\"pairs_encoded.json\",\"w\") as f:\n",
        "    json.dump(pairs_encoded, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "847f69cc",
      "metadata": {
        "id": "847f69cc"
      },
      "outputs": [],
      "source": [
        "class Mydataset(Dataset):\n",
        "    def __init__(self):\n",
        "        path = \"/content/pairs_encoded.json\"\n",
        "        self.pairs = json.load(open(path))\n",
        "        self.dataset_size = len(self.pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "#         we need to convert this list containin numbers to a long tensor\n",
        "        question = torch.LongTensor(self.pairs[idx][0]) \n",
        "        answer = torch.LongTensor(self.pairs[idx][1]) # this will retrieve one element from your dataset\n",
        "        return question, answer\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.dataset_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "babbb036",
      "metadata": {
        "id": "babbb036"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(Mydataset(),\n",
        "                                           batch_size = 100,\n",
        "                                          shuffle = True,\n",
        "                                          pin_memory = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "ccf42d22",
      "metadata": {
        "id": "ccf42d22"
      },
      "outputs": [],
      "source": [
        "question, answer = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "7ca703cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ca703cf",
        "outputId": "702bbfd8-234c-4dbf-df61-167e664a32a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([100, 25]), torch.Size([100, 27]))"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question.shape, answer.shape\n",
        "# answer has length of 27, because it has <start> and<end> token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "1b502fc9",
      "metadata": {
        "id": "1b502fc9"
      },
      "outputs": [],
      "source": [
        "# - sentence      : <start> I went to home last night <end>\n",
        "# - answer        : <start> I went to home last night , \"<start>\" is expected to predict \"I\", \"I\" is expected to predict \"went\", \"went\" is expected to predict \"to\"\n",
        "# - answer target :     I went to home last night <end>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "d9f0ff82",
      "metadata": {
        "id": "d9f0ff82"
      },
      "outputs": [],
      "source": [
        "def create_masks(question, answer, answer_target):\n",
        "    \n",
        "    def subsequent_mask(size):\n",
        "        mask = torch.triu(torch.ones(size,size)).transpose(0,1).type(dtype=torch.uint8) \n",
        "        # since we are working with 4D tensor, we need to unsequeeze it\n",
        "        return mask.unsqueeze(0)\n",
        "    \n",
        "    question_mask = ((question !=0).int()).to(device)\n",
        "    question_mask = question_mask.unsqueeze(1).unsqueeze(1) # (batch_size, 1, 1, max_words)\n",
        "    \n",
        "    answer_mask = ((answer !=0).int()).to(device)\n",
        "    answer_mask = answer_mask.unsqueeze(1) # (batch_size, 1, max_words)\n",
        "    answer_mask = answer_mask & subsequent_mask(answer.size(-1)).type_as(answer_mask.data) # answer.size(-1) = max_words\n",
        "#     (batch_size, max_words, max_words)\n",
        "    answer_mask = answer_mask.unsqueeze(1)\n",
        "    answer_target_mask = (answer_target !=0).int()\n",
        "    \n",
        "    return question_mask, answer_mask, answer_target_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "RxjrVHbk7u98",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxjrVHbk7u98",
        "outputId": "a1e9a122-733d-4488-e0e9-2ebcd2485876"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0], dtype=torch.int32)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(question[0]!=0).int()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c0b869",
      "metadata": {
        "id": "b4c0b869"
      },
      "source": [
        "#### Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "b2883558",
      "metadata": {
        "id": "b2883558"
      },
      "outputs": [],
      "source": [
        "# Create Embeddings\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, max_len=50):\n",
        "        super(Embeddings,self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_len = max_len\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.embed = nn.Embedding(self.vocab_size, self.embed_dim)\n",
        "        self.positional_encod = self.create_positional_encoding(self.max_len, self.embed_dim) # call function after creating\n",
        "        \n",
        "    def create_positional_encoding(self, max_len, embed_dim):\n",
        "        positional_encoding = torch.zeros(max_len, embed_dim).to(device)\n",
        "        for pos in range(max_len):\n",
        "            for i in range(0, embed_dim, 2):\n",
        "                positional_encoding[pos, i] = math.sin(pos / math.pow(10000,2*i/embed_dim) )\n",
        "                positional_encoding[pos, i+1] = math.cos(pos / math.pow(10000,2*i/embed_dim) )\n",
        "#                 we need to include batch size for this\n",
        "        positional_encoding = positional_encoding.unsqueeze(0) #results in shape (batch_size, max_len, embed_dim) --> (1, max_len, embed_dim),\n",
        "        return positional_encoding\n",
        "    \n",
        "    def forward(self, encoded_words):\n",
        "        embeddings = self.embed(encoded_words)*math.sqrt(self.embed_dim) # (batch_size, max_words, embed_dim)\n",
        "        # embeddings retrieve its embedding vector. These vectors will then be learnt as a parameters by the model,\n",
        "#         we wanna give more importance to the embeddings so we multiply is sqrt(self.embed_dim)\n",
        "#         max_words = embedding.size(1)\n",
        "        embeddings += self.positional_encod[:, :embeddings.size(1),] # pe will be automatically expanded to same batch_size as embeddings\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "0e798d2e",
      "metadata": {
        "id": "0e798d2e"
      },
      "outputs": [],
      "source": [
        "class Multi_Head_Attention(nn.Module):\n",
        "    \n",
        "    def __init__(self, heads, model_dim): # model_dim or embed_dim\n",
        "        super(Multi_Head_Attention, self).__init__()\n",
        "        assert model_dim % heads == 0\n",
        "        self.heads = heads\n",
        "        self.model_dim = model_dim\n",
        "        self.d_k = self.model_dim // self.heads\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.query = nn.Linear(self.model_dim, self.model_dim)\n",
        "        self.key = nn.Linear(self.model_dim, self.model_dim)\n",
        "        self.value = nn.Linear(self.model_dim, self.model_dim)\n",
        "        self.concat = nn.Linear(self.model_dim, self.model_dim)\n",
        "    \n",
        "    def forward(self,query, key, value, mask):\n",
        "        query = self.query(query) # (batch_size, max_len, model_dim) ----> (batch_size, max_len, 512)\n",
        "        key = self.key(key) # (batch_size, max_len, model_dim)\n",
        "        value = self.value(value) # (batch_size, max_len, model_dim)\n",
        "        \n",
        "#         (batch_size, max_words, 512) ---> (batch_size, max_words, 8, 64) ---> (batch_size, 8, max_words, d_k)\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3) # result shape:(batch_size, 8, max_words, h, d_k) \n",
        "        key = key.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3) # result shape:(batch_size, 8, max_words, d_k)\n",
        "        value = value.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3) # result shape:(batch_size, 8, max_words, d_k)\n",
        "        \n",
        "#         (batch_size, 8, max_words, d_k) dot (batch_size, 8, d_k, max_words) == result shape:(batch_size, 8 , max_words, max_words)\n",
        "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(self.d_k)  # result shape:(batch_size, 8 , max_words, max_words)\n",
        "        scores = scores.masked_fill(mask ==0, -1e9)\n",
        "        weights = F.softmax(scores, dim = -1)\n",
        "        weights = self.dropout(weights) # result shape:(batch_size, 8, max_words, max_words)\n",
        "        \n",
        "#         (batch_size, 8, max_words, max_words) dot value (batch_size, 8, max_words, d_k) ------> result shape: (batch_size, 8, max_words, d_k)\n",
        "        context = torch.matmul(weights,value) # result shape: (batch_size, 8, max_words, d_k)\n",
        "#         transpose it from (batch_size, 8, max_words, d_k) to (batch_size, max_words, 8 , d_k) and concatenate them (batch_size, max_words, 8*d_k)\n",
        "        context = context.permute(0,2,1,3).contiguous() # result shape: (batch_size, max_words, 8 , d_k)\n",
        "        context = context.view(context.shape[0], -1, self.heads*self.d_k) # result shape: (batch_size, max_words or max_len, 512 or model_dim = 512)\n",
        "        context_interacted = self.concat(context)\n",
        "        \n",
        "        return context_interacted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "1a32f352",
      "metadata": {
        "id": "1a32f352"
      },
      "outputs": [],
      "source": [
        "class Feed_Forward(nn.Module):\n",
        "    \n",
        "    def __init__(self,embed_dim, middle_dim = 2048):\n",
        "        super(Feed_Forward,self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.middle_dim = middle_dim\n",
        "        self.fc_linear1 = nn.Linear(self.embed_dim, self.middle_dim) #(512, 2048) --> result shape: 2048\n",
        "        self.fc_linear2 = nn.Linear(self.middle_dim, self.embed_dim) #(2048,512) --> result shape 512\n",
        "        self.dropout = nn.Dropout(0.1) # dropout rate used in the original paper\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.fc_linear1(x)\n",
        "        x = F.relu(x) # activation function used in the original paper\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc_linear2(x)\n",
        "        return x        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3c45e9f",
      "metadata": {
        "id": "f3c45e9f"
      },
      "source": [
        "#### d_model == embed_dim\n",
        "- In Encoder, we only have self_attention\n",
        "- In Decoder, we both have self_attention and source attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "b1771025",
      "metadata": {
        "id": "b1771025"
      },
      "outputs": [],
      "source": [
        "class Encoder_layer(nn.Module):\n",
        "    \n",
        "    def __init__(self,embed_dim, heads):\n",
        "        super(Encoder_layer,self).__init__()\n",
        "        \n",
        "        self.self_multihead_attention = Multi_Head_Attention(heads, embed_dim)\n",
        "        self.feed_forward = Feed_Forward(embed_dim, middle_dim= 2048)\n",
        "        self.layernorm = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, embeddings, mask):\n",
        "        interacted_context = self.self_multihead_attention(embeddings, embeddings, embeddings, mask)\n",
        "#         in transformer, self_multihead_attention( query, key, value, mask) takes these values normally\n",
        "        interacted_dropout = self.dropout(interacted_context)\n",
        "        interacted_normalized = self.layernorm(interacted_dropout + embeddings)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted_normalized))\n",
        "        encoded_output = self.layernorm(feed_forward_out + interacted_normalized )\n",
        "        return encoded_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "f12d8edd",
      "metadata": {
        "id": "f12d8edd"
      },
      "outputs": [],
      "source": [
        "class Decoder_layer(nn.Module):\n",
        "    \n",
        "    def __init__(self, embed_dim, heads):\n",
        "        super(Decoder_layer,self).__init__()\n",
        "        self.self_multihead_attention = Multi_Head_Attention(heads, embed_dim)\n",
        "        self.source_multihead_attention = Multi_Head_Attention(heads, embed_dim)\n",
        "        self.feed_forward = Feed_Forward(embed_dim, middle_dim= 2048)\n",
        "        self.layernorm = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "    \n",
        "    def forward(self, embeddings, encoded_output, source_mask, target_mask):\n",
        "        query = self.self_multihead_attention(embeddings, embeddings, embeddings, target_mask) # create query for source_attention\n",
        "        query = self.layernorm(query + embeddings) # outputs of the self attention plus the input, which is the embeddings\n",
        "#         keys and values as an input to source_attention comes from the encoded output\n",
        "        source_attention = self.source_multihead_attention(query, encoded_output, encoded_output, source_mask)\n",
        "        interacted_source_attention = self.dropout(source_attention)\n",
        "        interacted_source_attention = self.layernorm(interacted_source_attention + query)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted_source_attention))\n",
        "        decoded_output = self.layernorm(feed_forward_out + interacted_source_attention)\n",
        "        return decoded_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79926f0c",
      "metadata": {
        "id": "79926f0c"
      },
      "source": [
        "#### Build a full transformer model in py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "465f9106",
      "metadata": {
        "id": "465f9106"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    \n",
        "    def __init__(self, embed_dim, heads, num_layers, words_map):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.vocab_size = len(words_map)\n",
        "        self.embed  = Embeddings(self.vocab_size, self.embed_dim)\n",
        "#         N=num_layers / Encoder_layers are defined in original paper, N=6\n",
        "#         we have 6 Encoder_layers whose architerctures are the same but have different weights\n",
        "        self.encoder = nn.ModuleList([Encoder_layer(self.embed_dim, heads) for _ in range(num_layers)]) # assembles models in list\n",
        "        self.decoder = nn.ModuleList([Decoder_layer(self.embed_dim, heads) for _ in range(num_layers)])\n",
        "        self.logit = nn.Linear(self.embed_dim, self.vocab_size)\n",
        "        \n",
        "    def encode(self, source_words, source_mask): # source_words are questions\n",
        "        source_embeddings = self.embed(source_words)\n",
        "        for layer in self.encoder:\n",
        "            source_embeddings = layer(source_embeddings,source_mask)\n",
        "#             source_embeddings are fed into first encoder layer\n",
        "#             Here, there are 6 encoder layers, and output of the first encoder layer is fed into input of the second encoder layer and so it goes.\n",
        "        return source_embeddings\n",
        "\n",
        "    def decode(self, target_words, target_mask, source_embeddings, source_mask): # source_words are questions\n",
        "        target_embeddings = self.embed(target_words)\n",
        "        for layer in self.decoder:\n",
        "            target_embeddings = layer(target_embeddings, source_embeddings, source_mask, target_mask)\n",
        "#             source_embeddings are fed into first encoder layer\n",
        "#             Here, there are 6 encoder layers, and output of the first encoder layer is fed into input of the second encoder layer and so it goes.\n",
        "#             Here, both encoder and decoder share the same vocabulary and same words\n",
        "        return target_embeddings\n",
        "    \n",
        "    def forward(self, source_words, source_mask, target_words, target_mask):\n",
        "        encoded = self.encode(source_words, source_mask)\n",
        "        decoded = self.decode(target_words, target_mask, encoded, source_mask)\n",
        "#         why do we need log_softmax?\n",
        "#         we can ignore it if we train with crossentropy, we dont need to define log_softmax at all\n",
        "#         But here we use KL divergence loss, therefore we need to define softmax function\n",
        "        out = F.log_softmax(self.logit(decoded))\n",
        "#         -------------- additional information ---------------------\n",
        "#         pytorch automatically takes log and softmax of output logit if you use crossentropy\n",
        "#         out = self.logit(decoded)\n",
        "#         -----------------------------------------------------------\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "612d4a08",
      "metadata": {
        "id": "612d4a08"
      },
      "source": [
        "#### Now, we will define Adam optimization with warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "32dbcd4c",
      "metadata": {
        "id": "32dbcd4c"
      },
      "outputs": [],
      "source": [
        "class Adam_warmup:\n",
        "    \n",
        "    def __init__(self, model_size, warmup_steps, optimizer):\n",
        "        self.model_size = model_size\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.optimizer = optimizer\n",
        "        self.current_step = 0\n",
        "        self.lr = 0\n",
        "        \n",
        "    def get_lr(self):\n",
        "        return self.model_size**(-0.5)*min(self.current_step**(-0.5), self.current_step*self.warmup_steps**(-1.5))\n",
        "    \n",
        "    def step(self):\n",
        "        self.current_step += 1\n",
        "        lr = self.get_lr()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr   #update lr\n",
        "        self.lr = lr\n",
        "        self.optimizer.step() # update weights   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "712b83fd",
      "metadata": {
        "id": "712b83fd"
      },
      "source": [
        "- w < -- w - lr*gradient\n",
        "- optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "38261ae2",
      "metadata": {
        "id": "38261ae2"
      },
      "outputs": [],
      "source": [
        "class LosswithLS(nn.Module):\n",
        "    def __init__(self, size, smooth):\n",
        "        super(LosswithLS, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average= False, reduce= False) # The shape of matrix will be changed if reduce is True\n",
        "        self.smooth = smooth\n",
        "        self.confidence = 1 - self.smooth\n",
        "        self.size = size\n",
        "    \n",
        "    def forward(self, prediction, target, mask):\n",
        "        \"\"\"\n",
        "        prediction: (batch_size, max_words, vocab_size)\n",
        "        target and mask: (batch_size, max_words)\n",
        "        \"\"\"\n",
        "        prediction = prediction.view(-1,prediction.size(-1))\n",
        "        target = target.contiguous().view(-1)\n",
        "        mask = mask.float()\n",
        "        mask = mask.view(-1)\n",
        "        labels = prediction.data.clone()\n",
        "        labels.fill_(self.smooth/(self.size-1))\n",
        "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        loss = self.criterion(prediction, labels)  # shape (batch_size*max_words, vocab_size)\n",
        "        loss = (loss.sum(1)*mask).sum()/ mask.sum() # for example 1 0 1 1 0, ones are the words, 3/5\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e7baf4",
      "metadata": {
        "id": "31e7baf4"
      },
      "source": [
        "#### Visualize the operation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ba2fa5e",
      "metadata": {
        "id": "4ba2fa5e"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "6db06789",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6db06789",
        "outputId": "349bc314-a14c-43e6-cdc2-a56c586e148e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 3])"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size=2\n",
        "max_words=4\n",
        "vocab_size=3\n",
        "prediction = torch.randn(batch_size, max_words, vocab_size)\n",
        "prediction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "25b4407a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25b4407a",
        "outputId": "65ebdefb-8c53-47db-f88b-8461e93ea329"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 3])"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction = prediction.view(-1, prediction.shape[-1])\n",
        "prediction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "c9844cfe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9844cfe",
        "outputId": "17d4cf12-1e62-4228-8c11-0cbb4f0bbc6b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 2, 0, 2, 2, 0, 0, 2])"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target = torch.LongTensor(batch_size*max_words).random_(0,prediction.shape[-1])\n",
        "target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "5e1aa24b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e1aa24b",
        "outputId": "ccd738a9-98c8-4ef6-d79c-697cac0f6a0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8])"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask= target!=0\n",
        "mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "09c2b798",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09c2b798",
        "outputId": "6c14e57a-37a3-4360-f778-18bb7197d30e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 3])"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = prediction.data.clone()\n",
        "labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "cd3933f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd3933f5",
        "outputId": "42031951-a5f4-4543-95f0-e55b745410d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.1996, -1.3000,  0.5435],\n",
              "        [ 0.0533, -0.4221,  0.3832],\n",
              "        [-1.0267, -0.1952, -1.4813],\n",
              "        [-0.2746, -0.3783, -0.0335],\n",
              "        [-0.4049, -0.2981, -0.5315],\n",
              "        [-1.7057,  1.2303, -0.6768],\n",
              "        [ 1.3873,  0.1794,  0.3595],\n",
              "        [-0.4216, -0.5800, -0.0925]])"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "3eda081b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eda081b",
        "outputId": "b7485750-23d8-4427-e212-bc612ad69059"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.9000, -0.9000, -0.9000],\n",
              "        [-0.9000, -0.9000, -0.9000],\n",
              "        [-0.9000, -0.9000, -0.9000],\n",
              "        [-0.9000, -0.9000, -0.9000],\n",
              "        [-0.9000, -0.9000, -0.9000],\n",
              "        [-0.9000, -0.9000, -0.9000],\n",
              "        [-0.9000, -0.9000, -0.9000],\n",
              "        [-0.9000, -0.9000, -0.9000]])"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels.fill_(0.3/3-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "1aa7c062",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aa7c062",
        "outputId": "1767f66e-1c61-465a-d9ae-61d13f8a07e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 2, 0, 2, 2, 0, 0, 2])"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "977f4fb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "977f4fb4",
        "outputId": "b14d497e-4243-493c-b94c-3d37de12978b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.9000,  0.7000, -0.9000],\n",
              "        [-0.9000, -0.9000,  0.7000],\n",
              "        [ 0.7000, -0.9000, -0.9000],\n",
              "        [-0.9000, -0.9000,  0.7000],\n",
              "        [-0.9000, -0.9000,  0.7000],\n",
              "        [ 0.7000, -0.9000, -0.9000],\n",
              "        [ 0.7000, -0.9000, -0.9000],\n",
              "        [-0.9000, -0.9000,  0.7000]])"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels.scatter(1, target.data.unsqueeze(1), 1-0.3) # in order to use scatter, it has to be same dimension as target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "101456f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "101456f9",
        "outputId": "03b9c5c8-fba7-4620-8e4b-690a11b2cccd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 3])"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = torch.randn(batch_size*max_words, vocab_size)\n",
        "loss.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "3fff4418",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fff4418",
        "outputId": "af532b15-2a4a-4efe-b8da-cf445ce049f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.2021,  1.1412, -0.0658],\n",
              "        [-0.4760,  1.1799,  0.6341],\n",
              "        [-0.5906,  0.2444,  0.1670],\n",
              "        [-0.9773, -0.4466, -0.7714],\n",
              "        [ 2.0953, -1.0670, -0.5240],\n",
              "        [ 1.2094,  0.6666,  0.1277],\n",
              "        [-1.1101, -0.1664, -1.8733],\n",
              "        [ 0.9512,  1.2790, -1.5753]])"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "9ada7231",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ada7231",
        "outputId": "c8b3aa2c-7a54-49e8-b826-bd776b2899bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 1.2774,  1.3380, -0.1792, -2.1953,  0.5043,  2.0037, -3.1499,  0.6550])"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss.sum(1)  # we sum of the elements along the direction dim =1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "4a2576f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a2576f9",
        "outputId": "f7c072ea-dd2a-4257-cd13-a4ae65f3e580"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ True,  True, False,  True,  True, False, False,  True])"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "bd20f9e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd20f9e8",
        "outputId": "e316dab2-6b9b-4468-cc17-2381c4c3f89e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([8, 3]), torch.Size([8]))"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss.shape, mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "e76aa081",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e76aa081",
        "outputId": "51dc7699-6d4d-4d97-bf6d-7b0fc031535a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 1.2774,  1.3380, -0.0000, -2.1953,  0.5043,  0.0000, -0.0000,  0.6550])"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss.sum(1)*mask.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "84a826d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84a826d8",
        "outputId": "a99fa319-d0e9-43f4-e304-1f00894eaa21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.3159)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(loss.sum(1)*mask.float()).sum() / mask.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7562ff1",
      "metadata": {
        "id": "b7562ff1"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "d95751b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d95751b1",
        "outputId": "535aeda5-b26d-422d-8811-08352dc37467"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ],
      "source": [
        "embed_dim = 512\n",
        "heads = 8\n",
        "num_layers = 4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "epochs = 100\n",
        "\n",
        "with open(\"/content/Wordmap_corpus.json\",\"r\") as f:\n",
        "    words_map = json.load(f)\n",
        "    \n",
        "transformer = Transformer( embed_dim= embed_dim,heads= heads, num_layers= num_layers, words_map= words_map)\n",
        "transformer.to(device)\n",
        "\n",
        "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = Adam_warmup(embed_dim, warmup_steps=4000, optimizer=adam_optimizer)\n",
        "criterion = LosswithLS(size=len(words_map), smooth=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9902a932",
      "metadata": {
        "id": "9902a932"
      },
      "source": [
        "### Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "2df6a493",
      "metadata": {
        "id": "2df6a493"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, transformer, criterion, epoch):\n",
        "    \n",
        "    transformer.train() # we need to set it training mode, because we are using dropout\n",
        "    # dropout acts differently in training and testing time.\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "    \n",
        "    for i, (question, answer) in enumerate(train_loader):\n",
        "        samples = question.shape[0] # retrieve the batch size\n",
        "        question = question.to(device)\n",
        "        answer = answer.to(device)\n",
        "        \n",
        "        # at a training time: for instance sentence: <start> A man is playing with a dog <end>  -----> Sentence\n",
        "        answer_input = answer[:,:-1] # last word is <end> token, we dont take it, example: <start> A man is playing with a dog -----> Input\n",
        "        answer_target = answer[:, 1:] # Beginning word is <start> token, we dont take it, example: A man is playing with a dog <end>  -----> Target\n",
        "        question_mask, answer_mask, answer_target_mask = create_masks(question, answer_input, answer_target)\n",
        "        \n",
        "        out = transformer(question, question_mask,answer_input, answer_mask) # now, we have our predictions\n",
        "        loss = criterion(out, answer_target, answer_target_mask)\n",
        "        #backpropagation\n",
        "        transformer_optimizer.optimizer.zero_grad() # we dont want to accumulate gradients\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "        sum_loss += loss.item() * samples # using item(), because we are getting value from tensor\n",
        "        count += samples # at the last batch for an epoch, the count = len(train_loader)*100\n",
        "        \n",
        "        if i % 100 ==0:\n",
        "            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss / count))\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eba0b1a",
      "metadata": {
        "id": "8eba0b1a"
      },
      "source": [
        "### Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "65a0b63c",
      "metadata": {
        "id": "65a0b63c"
      },
      "outputs": [],
      "source": [
        "def evaluate(transformer, question, question_mask, max_len, words_map):\n",
        "    \"\"\"\n",
        "    This function is gonna perform greedy search decoding with a batch size of 1\n",
        "    \"\"\"\n",
        "    rev_words_map = { index : word for word, index in words_map.items() }\n",
        "    transformer.eval()\n",
        "    start_token = words_map[\"<start>\"]\n",
        "#     def encode(self, source_words, source_mask):\n",
        "    encoded = transformer.encode(question, question_mask)\n",
        "    words = torch.LongTensor([[start_token]]).to(device) # shape: (1, 1)\n",
        "    \n",
        "    for step in range(max_len-1):\n",
        "        size = words.shape[1]\n",
        "        target_mask = torch.triu(torch.ones(size,size)).transpose(0,1).type(dtype=torch.uint8)\n",
        "        target_mask = target_mask.to(device).unsqueeze(0)\n",
        "#         def decode(self, target_words, target_mask, source_embeddings, source_mask):\n",
        "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
        "#         decoded shape : (batch_size, max_len, vocab_size) ---> (1, 1, vocab_size)\n",
        "        predictions = transformer.logit(decoded[:, -1]) # we only take the last element, outout of the work that you are about to generate\n",
        "#         predictions shape: (1, vocab_size)\n",
        "        _, next_word = torch.max(predictions, dim =1) # next_word shape:(1, 1)\n",
        "        next_word = next_word.item()\n",
        "        if next_word == words_map[\"<end>\"]:\n",
        "            break\n",
        "        words = torch.cat( [words, torch.LongTensor([[next_word]]).to(device)] , dim =1) #shape: (1, step+2)\n",
        "    \n",
        "    #words shape will be like (1, 5), so we will get rid of first dimension\n",
        "    words = words.squeeze(0) #resulting dimension is 1 dimensional tensor with 5 elements, each of which will be a predicted word.\n",
        "    words = words.tolist() #convert tensor to list\n",
        "    \n",
        "    # build sentence\n",
        "    sentence_idx = [w for w in words if w not in {words_map[\"<start>\"]}]\n",
        "    sentence = \" \".join([ rev_words_map[sentence_idx[i]] for i in range(len(sentence_idx)) ])\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "aqP9c6W65Zwp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqP9c6W65Zwp",
        "outputId": "7d7887a3-cdc1-46d0-d9e2-ae6a0d7319f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "86aa2673",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86aa2673",
        "outputId": "26941550-36e6-46eb-98e9-cf9fa58e12d2"
      },
      "outputs": [],
      "source": [
        "# you can start training by running this cell\n",
        "for epoch in range(epochs):\n",
        "  train(train_loader, transformer, criterion, epoch)\n",
        "  state = {\"epoch\": epoch, \"transformer\" : transformer, \"transformer_optimizer\": transformer_optimizer}\n",
        "  torch.save(state, \"checpoint_\" + str(epoch) + \".tar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d76c6cc7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "d76c6cc7",
        "outputId": "de2015c9-2623-4cfc-eb1d-922b2c362b76"
      },
      "outputs": [],
      "source": [
        "# after you train the transformer, you can choose checkpoint\n",
        "checkpoint = torch.load(\"checkpoint_.tar\")\n",
        "transformer = checkpoint[\"transformer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8385dd9",
      "metadata": {
        "id": "c8385dd9"
      },
      "outputs": [],
      "source": [
        "while(1):\n",
        "  question = input(\"Question: \")\n",
        "  if question == \"quit\":\n",
        "    break\n",
        "  \n",
        "  max_len = input(\"Enter max words to be generated: \")\n",
        "  encoded_qeustion = [ words_map.get(word, words_map[\"<unknown>\"]) for word in question.split()]\n",
        "  question = torch.LongTensor(encoded_qeustion).to(device).unsqueeze(0)\n",
        "  question = (question !=0).to(device).unsqueeze(1).unsqueeze(1)\n",
        "  sentence = evaluate(transformer, question, question_mask, int(max_len),words_map)\n",
        "  print(sentence)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.6 ('pythonProject')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "49a989f7cfe4b8da2a59a725bbf71baebbe1b42e64c745a96da729d2da270941"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
